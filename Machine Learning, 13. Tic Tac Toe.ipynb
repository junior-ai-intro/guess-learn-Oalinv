{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 13. Tic-Tac-Toe</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***13. Tic-tac-toe***\n",
    "\n",
    "Tic-tac-toe doesn't need an introduction. Let's just play.\n",
    "\n",
    "Make a move by typing the number of the space that you want to claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import *\n",
    "#\n",
    "# we'll let the human player go first\n",
    "#\n",
    "g = Game(HumanPlayer(), MinimaxPlayer())\n",
    "g.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import *\n",
    "#\n",
    "# now we'll let the computer player go first.. \n",
    "# the first move might take a little while.\n",
    "#\n",
    "g = Game(MinimaxPlayer(), HumanPlayer())\n",
    "g.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so with a little practice, the best we can hope for is a tie game, because we are playing against a __Minimax__ player.\n",
    "\n",
    "Who is this __MinimaxPlayer__ who knows so much about tic-tac-toe?\n",
    "\n",
    "_And if this course is called __Guessing Games__, why play tic-tac-toe at all? What does that have to do with guessing?_\n",
    "\n",
    "(and why are you asking so many questions all at once?)\n",
    "\n",
    "Let's start with __Minimax__:\n",
    "\n",
    "<font face=Times size=3><bold><blockquote>\n",
    "    The Minimax algorithm is the most well-known strategy of play of two-player, zero-sum games. The minimax theorem was proven by John von Neumann in 1928. Minimax is a strategy of always minimizing the maximum possible loss which can result from a choice that a player makes.<p><br>\n",
    "    Roberts, _Strategies of Play_, Stanford University Computer Science (1998-1999)\n",
    "</blockquote></bold></font>\n",
    "Or, one step at a time:\n",
    "- a __two-player, zero-sum-game__ is a game where a player wins only by causing another player to lose.\n",
    "- other examples of two-player, zero-sum games would be __singles tennis__ or __the cold war__.\n",
    "- __John von Neumann__ was the Einstein of math, game theory, and computer science.\n",
    "- __Minimax__ says \"if you can't win right now, prevent your opponent from winning.\"\n",
    "- __Minimax__ tries every possible move, and every possible response, every time.\n",
    "- to function, __Minimax__ needs to know the rules of the game (in order to try every move).\n",
    "\n",
    "In practice, a half-finished pass through the __Minimax__ tic-tac-toe algorithm looks something like this:\n",
    "<img src=\"minimax-animation.png\">\n",
    "(image generated at http://www.algomation.com)<p>\n",
    "__Minimax__ is the opposite of _machine learning_. Remember that we defined _machine learning_ as _the art of accumulating knowledge by learning from mistakes_? __Minimax__ doesn't make any mistakes. Ever.\n",
    "\n",
    "So how can machine learning ever hope to defeat the evil __Minimax__?\n",
    "\n",
    "(or OK, at least play to draw?)\n",
    "\n",
    "Let's start with a look at a tic-tac-toe environment, which can present itself just like the maze, using __reset()__, __sample()__, and __step()__. To keep things simple, the tic-tac-toe environment always allows the machine learning player to play __X__ and therefore move first.\n",
    "\n",
    "Let's start by retrieving our initial state, using __reset()__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import *\n",
    "\n",
    "########################################\n",
    "#                                      #\n",
    "# The TicTacToe framework can present  #\n",
    "# itself just like the maze... using   #\n",
    "# reset(), sample(), and step(action)  #\n",
    "#                                      #\n",
    "########################################\n",
    "\n",
    "g = Game(EmptyPlayer()) # specifying only an opponent sets up system for reinforcement learning\n",
    "state = g.reset()\n",
    "print('the problem has', Game.state_space(), 'states; initial state =', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. We are in state 9841 out of 19683 possible states.\n",
    "\n",
    "The good news is: when taking a machine learning approach, we don't care which state is which, or which one comes first (provided that our computer has enough memory to record them all).\n",
    "\n",
    "But for the curious: there are 9 squares, and each square is either __X__, __O__, or __empty__; the framework assigns discrete values: __X=2, empty=1, O=0__, the state is represented as a 9-digit number, in base 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't need to know these details to solve the problem!\n",
    "# it's here to avoid the occasional github post saying: how do the states work...?\n",
    "\n",
    "def calculate_state(board):\n",
    "    sum = 0\n",
    "    for n in range(len(board)):\n",
    "        sum += board[n] * 3**n\n",
    "    return sum\n",
    "\n",
    "print(\"all O's   =\",calculate_state([0,0,0,0,0,0,0,0,0]))\n",
    "print(\"all X's   =\",calculate_state([2,2,2,2,2,2,2,2,2]))\n",
    "print(\"all empty =\",calculate_state([1,1,1,1,1,1,1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we _really need to know_ is that our problem has a finite number of states and actions, and we can rely on the framework to provide a sample action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import *\n",
    "g = Game()\n",
    "print('total states =', Game.state_space(), 'total actions =', Game.action_space())\n",
    "for n in range(10):\n",
    "    print('sample action:', g.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by playing a game, taking random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import *\n",
    "env = Game()\n",
    "env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.sample()\n",
    "    state, reward, done = env.step(action)\n",
    "    print('==================================================================')\n",
    "    print('action=',action,'state,reward,done=',state, reward, done, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "***Exercises***<p>\n",
    "\n",
    "Play 100 games, making random moves; record the outcome of each game in a __q-table__. How many cells of the __q-table__ contain positive, negative, or zero values?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tictactoe import *\n",
    "\n",
    "#######################################################\n",
    "#                                                     #\n",
    "#  Allocate space for your q-table here...            #\n",
    "#  ...the dimensions are: state_space x action_space  #\n",
    "#                                                     #\n",
    "#######################################################\n",
    "\n",
    "env = Game()\n",
    "for n in range(100):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.sample()\n",
    "        new_state, reward, done = env.step(action)\n",
    "        ###############################################\n",
    "        #                                             #\n",
    "        # Accumulate outcomes into the q-table here,  #\n",
    "        # based on the state preceding the action;    #\n",
    "        # use the new_state on the next pass through  #\n",
    "        # the loop.                                   #\n",
    "        #                                             #\n",
    "        ###############################################\n",
    "        print(n)\n",
    "print(n, np.max(q), np.min(q), np.sum(q))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
