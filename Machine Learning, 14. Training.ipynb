{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 14. Training</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***14. Training***\n",
    "\n",
    "Remember the maze? When we figured out how to __explore__ versus __exploit__ the experience represented in a __q-table__? And remember we decided to __discount__ future rewards?\n",
    "\n",
    "Let's build those elements into an algorithm that _trains itself_ to play tic-tac-toe.\n",
    "\n",
    "__And here's the big idea: there is nothing specific to tic-tac-toe in the algorithm. It may as well be solving a maze, or some other problem. The algorithm just explores & exploits based on states, transitions, and resulting rewards. _It doesn't even know the rules of the game._ __\n",
    "\n",
    "Here is an alogrithm that can learn to play, or solve the maze, or lots of things (it will take several minutes to run -- and will stop when it has trained itself to play the game):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tictactoe import *\n",
    "\n",
    "env = Game()                # the environment happens to be a game, but could be a maze or a puzzle...\n",
    "\n",
    "discount = 0.9              # these two variables control how we store rewards or penalties in our\n",
    "explore = 0.01              # q-table, just like in the lessons about the maze.\n",
    "\n",
    "#######################################################################################################\n",
    "#                                                                                                     #\n",
    "# SUPER IMPORTANT POINT: we don't even need to know the dimensions of the problem (the total number   #\n",
    "#                        of states of actions... we can ask the environment to tell us those limits)  #\n",
    "#                                                                                                     #\n",
    "q = np.zeros((Game.state_space(), Game.action_space()))                                               #                                       \n",
    "#                                                                                                     #\n",
    "#######################################################################################################\n",
    "\n",
    "attempts = 0                # keep track of how many times we have tried to solve the problem\n",
    "not_yet_trained = True      # and keep trying until we are 'trained', see below\n",
    "\n",
    "while not_yet_trained:\n",
    "    total_reward = 0\n",
    "    attempts += 1\n",
    "    for n in range(10):                                    # play 10 games, keep track of the total reward\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ###############################################################################################\n",
    "        #                                                                                             #\n",
    "        # The actual q-learning is exactly like the example with the maze                             #\n",
    "        #                                                                                             #\n",
    "        while not done:                                                                               #\n",
    "            if np.random.random() < explore:                                                          #\n",
    "                action = env.sample()                                                                 #\n",
    "            else:                                                                                     #\n",
    "                action = np.argmax(q[state])                                                          #\n",
    "            new_state, reward, done = env.step(action)                                                #\n",
    "            q[state][action] += reward + discount * np.amax(q[new_state])                             #\n",
    "            state = new_state                                                                         #\n",
    "        #                                                                                             #\n",
    "        ###############################################################################################      \n",
    "        total_reward += reward\n",
    "    not_yet_trained = True if total_reward < 0 else False  # if we never lose in 10 games, we are 'trained'\n",
    "    print(attempts, total_reward)                          # print a progress report\n",
    "        \n",
    "env.replay()                                               # print a replay of the last game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "***Exercises***\n",
    "\n",
    "Take a good look at the example, you will need it for the final project. Try tuning __discount__ and __explore__. How does the pace of training depend on these variables? Should they be changed within the program, as the success rate improves?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
