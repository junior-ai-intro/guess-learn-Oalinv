{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 14. Training</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***14. Training***\n",
    "\n",
    "Remember the maze? When we figured out how to __explore__ versus __exploit__ the experience represented in a __q-table__? And remember we decided to __discount__ future rewards?\n",
    "\n",
    "Let's build those elements into an algorithm that _trains itself_ to play tic-tac-toe.\n",
    "\n",
    "__And here's the big idea: there is nothing specific to tic-tac-toe in the algorithm. It may as well be solving a maze, or some other problem. The algorithm just explores & exploits based on states, transitions, and resulting rewards. _It doesn't even know the rules of the game._ __\n",
    "\n",
    "Here is an alogrithm that can learn to play, or solve the maze, or lots of things (it will take several minutes to run -- and will stop when it has trained itself to play the game):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -10\n",
      "2 -10\n",
      "3 -1\n",
      "4 -1\n",
      "5 0\n",
      "=== REPLAY =================================\n",
      "[9842, 9921, 9764, 9927, 10484, 9225, 10700, 11169, 15074]\n",
      "===( 1 [ state=9842 ] )=====================\n",
      "\n",
      "\\ /               \n",
      " X     1     2    \n",
      "/ \\               \n",
      "\n",
      "                  \n",
      " 3     4     5    \n",
      "                  \n",
      "\n",
      "                  \n",
      " 6     7     8    \n",
      "                  \n",
      "\n",
      "\n",
      "===( 2 [ state=9921 ] )=====================\n",
      "\n",
      "OOO               \n",
      "O O    1     2    \n",
      "OOO               \n",
      "\n",
      "      \\ /         \n",
      " 3     X     5    \n",
      "      / \\         \n",
      "\n",
      "                  \n",
      " 6     7     8    \n",
      "                  \n",
      "\n",
      "\n",
      "===( 3 [ state=9764 ] )=====================\n",
      "\n",
      "\\ /   \\ /         \n",
      " X     X     2    \n",
      "/ \\   / \\         \n",
      "\n",
      "      OOO         \n",
      " 3    O O    5    \n",
      "      OOO         \n",
      "\n",
      "                  \n",
      " 6     7     8    \n",
      "                  \n",
      "\n",
      "\n",
      "===( 4 [ state=9927 ] )=====================\n",
      "\n",
      "OOO   OOO   \\ /   \n",
      "O O   O O    X    \n",
      "OOO   OOO   / \\   \n",
      "\n",
      "      \\ /         \n",
      " 3     X     5    \n",
      "      / \\         \n",
      "\n",
      "                  \n",
      " 6     7     8    \n",
      "                  \n",
      "\n",
      "\n",
      "===( 5 [ state=10484 ] )=====================\n",
      "\n",
      "\\ /   \\ /   OOO   \n",
      " X     X    O O   \n",
      "/ \\   / \\   OOO   \n",
      "\n",
      "      OOO         \n",
      " 3    O O    5    \n",
      "      OOO         \n",
      "\n",
      "\\ /               \n",
      " X     7     8    \n",
      "/ \\               \n",
      "\n",
      "\n",
      "===( 6 [ state=9225 ] )=====================\n",
      "\n",
      "OOO   OOO   \\ /   \n",
      "O O   O O    X    \n",
      "OOO   OOO   / \\   \n",
      "\n",
      "\\ /   \\ /         \n",
      " X     X     5    \n",
      "/ \\   / \\         \n",
      "\n",
      "OOO               \n",
      "O O    7     8    \n",
      "OOO               \n",
      "\n",
      "\n",
      "===( 7 [ state=10700 ] )=====================\n",
      "\n",
      "\\ /   \\ /   OOO   \n",
      " X     X    O O   \n",
      "/ \\   / \\   OOO   \n",
      "\n",
      "OOO   OOO   \\ /   \n",
      "O O   O O    X    \n",
      "OOO   OOO   / \\   \n",
      "\n",
      "\\ /               \n",
      " X     7     8    \n",
      "/ \\               \n",
      "\n",
      "\n",
      "===( 8 [ state=11169 ] )=====================\n",
      "\n",
      "OOO   OOO   \\ /   \n",
      "O O   O O    X    \n",
      "OOO   OOO   / \\   \n",
      "\n",
      "\\ /   \\ /   OOO   \n",
      " X     X    O O   \n",
      "/ \\   / \\   OOO   \n",
      "\n",
      "OOO   \\ /         \n",
      "O O    X     8    \n",
      "OOO   / \\         \n",
      "\n",
      "\n",
      "===( 9 [ state=15074 ] )=====================\n",
      "\n",
      "\\ /   \\ /   OOO   \n",
      " X     X    O O   \n",
      "/ \\   / \\   OOO   \n",
      "\n",
      "OOO   OOO   \\ /   \n",
      "O O   O O    X    \n",
      "OOO   OOO   / \\   \n",
      "\n",
      "\\ /   OOO   \\ /   \n",
      " X    O O    X    \n",
      "/ \\   OOO   / \\   \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tictactoe import *\n",
    "\n",
    "env = Game()                # the environment happens to be a game, but could be a maze or a puzzle...\n",
    "\n",
    "discount = 0.9              # these two variables control how we store rewards or penalties in our\n",
    "explore = 0.01              # q-table, just like in the lessons about the maze.\n",
    "\n",
    "#######################################################################################################\n",
    "#                                                                                                     #\n",
    "# SUPER IMPORTANT POINT: we don't even need to know the dimensions of the problem (the total number   #\n",
    "#                        of states of actions... we can ask the environment to tell us those limits)  #\n",
    "#                                                                                                     #\n",
    "q = np.zeros((env.state_space(), env.action_space()))                                                 #                                       \n",
    "#                                                                                                     #\n",
    "#######################################################################################################\n",
    "\n",
    "attempts = 0                # keep track of how many times we have tried to solve the problem\n",
    "not_yet_trained = True      # and keep trying until we are 'trained', see below\n",
    "\n",
    "while not_yet_trained:\n",
    "    total_reward = 0\n",
    "    attempts += 1\n",
    "    for n in range(10):                                    # play 10 games, keep track of the total reward\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ###############################################################################################\n",
    "        #                                                                                             #\n",
    "        # The actual q-learning is exactly like the example with the maze                             #\n",
    "        #                                                                                             #\n",
    "        while not done:                                                                               #\n",
    "            if np.random.random() < explore:                                                          #\n",
    "                action = env.sample()                                                                 #\n",
    "            else:                                                                                     #\n",
    "                action = np.argmax(q[state])                                                          #\n",
    "            new_state, reward, done = env.step(action)                                                #\n",
    "            q[state][action] += reward + discount * np.amax(q[new_state])                             #\n",
    "            state = new_state                                                                         #\n",
    "        #                                                                                             #\n",
    "        ###############################################################################################      \n",
    "        total_reward += reward\n",
    "    not_yet_trained = True if total_reward < 0 else False  # if we never lose in 10 games, we are 'trained'\n",
    "    print(attempts, total_reward)                          # print a progress report\n",
    "        \n",
    "env.replay()                                               # print a replay of the last game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "***Exercises***\n",
    "\n",
    "Take a good look at the example, you will need it for the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
