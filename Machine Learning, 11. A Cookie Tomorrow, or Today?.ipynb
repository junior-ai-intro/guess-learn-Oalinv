{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 11. A cookie tomorrow, or today?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11. A cookie tomorrow, or today?**\n",
    "\n",
    "What's more valuabe: a cookie tomorrow, or a cookie today?\n",
    "\n",
    "Here is a q-table (showing only the cumulative rewards, and excluding the penalities) for an exploration of the maze using the algorithm from the last lesson. In this case, one of the random steps happened to go from east to west, then back again, and by doing that, accidently captured the _future reward_ for a meaningless move, potentially creating a loop in our optimum path:\n",
    "<pre>\n",
    "=====  ================================\n",
    "state         N       S       E       W\n",
    "\n",
    "(0,0)                     1.000        \n",
    "(0,1)                     1.000   1.000    < - Why am I indifferent between choosing East or West?\n",
    "(0,2)             1.000           1.000\n",
    "(0,3)                                  \n",
    "\n",
    "(1,0)     1.000                        \n",
    "(1,1)                                  \n",
    "(1,2)                     2.000        \n",
    "(1,3)             2.000           1.000\n",
    "\n",
    "(2,0)                                  \n",
    "(2,1)                                  \n",
    "(2,2)                                  \n",
    "(2,3)             1.000                \n",
    "\n",
    "(3,0)                                  \n",
    "(3,1)                                  \n",
    "(3,2)                                  \n",
    "(3,3)                                  \n",
    "                               \n",
    "\n",
    "</pre>\n",
    "\n",
    "Why did that happen? Because in our algorithm, _all rewards are created equal_, meaning we give equal weight to a _current reward_ (finding the exit), or a _future reward_ (finding a step that leads toward a future step that finds the exit). If we happen to move back and forth between two steps, we add any positive reward from one to the other, and back again, falsely creating a loop, and potentially amplifying the reward.\n",
    "\n",
    "That problem is easily solved by _discounting future rewards_. __Discounting__ means \"giving less than 100% credit to a reward that you expect in the future.\"\n",
    "\n",
    "**Or: a cookie tomorrow is a good thing, but it's worth less than a cookie today.**\n",
    "\n",
    "Here is a version of our solution from the prior lesson, changed to solve the maze 10 times (potentially amplifying rewards). Run it several times to see __q-tables__ that have potential loops in the resulting rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "maze = Maze()\n",
    "q = np.zeros((4,4,4))\n",
    "solved = False\n",
    "while not solved:\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = maze.sample_n()                         \n",
    "        new_state, reward, done = maze.step(action)\n",
    "        q[state[0]][state[1]][action] += reward\n",
    "        if max(new_state) < 4 and min(new_state) >= 0:\n",
    "            q[state[0]][state[1]][action] += max(q[new_state[0]][new_state[1]])\n",
    "        state = new_state\n",
    "        solved = True if max(q[0][0]) > 10 else False\n",
    "\n",
    "Maze.print_q(np.maximum(q,0), mode='rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "***Exercises***<p>\n",
    "Correct the program to value future rewards at 1/2 the maximum value of the reward available for the future step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "###############################################\n",
    "#                                             #\n",
    "#  Correct the program to conform to the      #\n",
    "#  sample output (below). Hint: you can       #\n",
    "#  complete this exercise by typing three     #\n",
    "#  or four characters into an existing line   #\n",
    "#  of code.                                   #\n",
    "#                                             #\n",
    "###############################################\n",
    "\n",
    "'''\n",
    "SAMPLE OUTPUT\n",
    "\n",
    "Here is sample output when the future reward\n",
    "is valued at 1/2 the maximum value of the next\n",
    "step:\n",
    "\n",
    "=====  ================================\n",
    "state         N       S       E       W\n",
    "\n",
    "(0,0)                     0.031        \n",
    "(0,1)                     0.062   0.016\n",
    "(0,2)             0.125                \n",
    "(0,3)                                  \n",
    "-----  --------------------------------\n",
    "(1,0)                                  \n",
    "(1,1)                                  \n",
    "(1,2)                     0.250        \n",
    "(1,3)             0.500                \n",
    "-----  --------------------------------\n",
    "(2,0)                                  \n",
    "(2,1)                                  \n",
    "(2,2)                                  \n",
    "(2,3)             1.000                \n",
    "-----  --------------------------------\n",
    "(3,0)                                  \n",
    "(3,1)                                  \n",
    "(3,2)                                  \n",
    "(3,3)                                  \n",
    "-----  --------------------------------\n",
    "'''\n",
    "\n",
    "maze = Maze()\n",
    "q = np.zeros((4,4,4))\n",
    "solved = False\n",
    "while not solved:\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = maze.sample_n()                         \n",
    "        new_state, reward, done = maze.step(action)\n",
    "        q[state[0]][state[1]][action] += reward\n",
    "        if max(new_state) < 4 and min(new_state) >= 0:\n",
    "            q[state[0]][state[1]][action] += max(q[new_state[0]][new_state[1]])\n",
    "        state = new_state\n",
    "        solved = True if max(q[0][0]) > 0 else False   # run until the rewards pile up at (0,0)\n",
    "\n",
    "Maze.print_q(np.maximum(q,0), mode='rewards')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
