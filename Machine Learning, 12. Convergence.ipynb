{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 12. Convergence</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12. Convergence**<p>\n",
    "\n",
    "<font face=Times size=3><bold><blockquote>\n",
    "_In mathematics, computer science and logic, convergence refers to the idea that different sequences of transformations come to a conclusion in a finite amount of time (the transformations are terminating), and that the conclusion reached is independent of the path taken to get to it (they are confluent)._<p>\n",
    "\n",
    "Franz Baader; Tobias Nipkow (1998). _Term Rewriting and All That._ Cambridge University Press (via Wikipedia)\n",
    "</blockquote></bold></font>\n",
    "\n",
    "Let's balance _exploration_ and _exploitation_ by defining a constant called __explore__. Every time we take a step in the maze, we will either _explore_ or _exploit_, by comparing a random number to __explore__... something like this:\n",
    "<pre>\n",
    "if random_number < explore:\n",
    "    explore the maze using sample(), as in the prior examples\n",
    "else:\n",
    "    using everything that we have learned to pick the best next step\n",
    "</pre>\n",
    "\n",
    "If __explore__ = 0.10, we will explore the maze 10% of the time, and rely on our results 90% of the time. The idea is to learn as we go, eventually finding the exit on every attempt. The mathematical journey of exploring many possibilities that, in time, resolve to a single solution is an example of _convergence_.\n",
    "\n",
    "Using the code sample, below, try using different values for __discount__ and __explore__, to see if the results begin to converge. What happens to runs that find the exit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "##########################################################################\n",
    "#                                                                        #   \n",
    "#  RUN SOME EXPERIMENTS BY CHANGING THE VALUES FOR discount AND explore  #\n",
    "#  don't get discouraged if the values go a little crazy                 #\n",
    "#                                                                        #\n",
    "#  The last lesson effectively used:                                     #\n",
    "#      discount = 0.5                                                    #\n",
    "#      explore  = 1.0                                                    #\n",
    "#                                                                        #\n",
    "discount = 0.50                                                         \n",
    "explore  = 0.50                                                         \n",
    "#                                                                        #\n",
    "##########################################################################\n",
    "\n",
    "maze = Maze(record=True)\n",
    "q = np.zeros((4,4,4))\n",
    "\n",
    "for n in range(1000):\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:     \n",
    "        if np.random.random() < explore:\n",
    "            # explore the maze using a random action\n",
    "            action = maze.sample_n()\n",
    "            new_state, reward, done = maze.step(action)\n",
    "            q[state[0]][state[1]][action] += reward\n",
    "            if max(new_state) < 4 and min(new_state) >= 0:\n",
    "                q[state[0]][state[1]][action] += discount * max(q[new_state[0]][new_state[1]])\n",
    "            state = new_state\n",
    "        else:\n",
    "            # exploit past attempts by taking the highest value action\n",
    "            state, _, done = maze.step(np.argmax(q[state[0]][state[1]]))\n",
    "\n",
    "# plot convergence (distance to exit for each successive attempt)\n",
    "maze.convergence()\n",
    "Maze.print_q(q,mode='rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the values of __discount__ and __explore__, the contents of the __q-table__ may get a little out of control. The positive rewards get added together (even with a discount < 1.0), and can easily go out of bounds. The whole goal is to find our way from the entrance to the exit, so how about trying this:\n",
    "\n",
    "_When we receive a positive reward at our initial state, stop exploring! ...we now know the solution._\n",
    "\n",
    "(that means that we found the one-and-only __reward__ at the exit, then that positive q-value worked its way across our __q-table__ backwards, all the way to the entrance, over a series of subsequent attempts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "##########################################################################\n",
    "#                                                                        #   \n",
    "#  RUN SOME EXPERIMENTS BY CHANGING THE VALUES FOR discount AND explore  #\n",
    "#  don't get discouraged if the values go a little crazy                 #\n",
    "#                                                                        #\n",
    "#  The last lesson effectively used:                                     #\n",
    "#      discount = 0.5                                                    #\n",
    "#      explore  = 1.0                                                    #\n",
    "#                                                                        #\n",
    "discount =  0.50                                                        \n",
    "explore  =  0.50\n",
    "#                                                                        #\n",
    "##########################################################################\n",
    "\n",
    "maze = Maze(record=True)\n",
    "q = np.zeros((4,4,4))\n",
    "\n",
    "for n in range(1000):\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:     \n",
    "        if np.random.random() < explore:\n",
    "            action = maze.sample_n()\n",
    "            new_state, reward, done = maze.step(action)\n",
    "            q[state[0]][state[1]][action] += reward\n",
    "            if max(new_state) < 4 and min(new_state) >= 0:\n",
    "                q[state[0]][state[1]][action] += discount * max(q[new_state[0]][new_state[1]])\n",
    "            state = new_state\n",
    "        else:\n",
    "            state, _, done = maze.step(np.argmax(q[state[0]][state[1]]))\n",
    "            \n",
    "        # if we can find the exit from the starting point, stop exploring!\n",
    "        if max(q[0][0]) > 0:\n",
    "            explore = 0\n",
    "\n",
    "# plot convergence (distance to exit for each successive attempt)\n",
    "maze.convergence()\n",
    "Maze.print_q(q,mode='rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "***Exercises***<p>\n",
    "Reconfigure this program to converge, on average, in the fewest possible steps (don't take 'fewest' literally... it's still a random process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "discount =  0.50    # try changing this constant                                             \n",
    "explore  =  0.50    # try changing this constant\n",
    "\n",
    "maze = Maze(record=True)\n",
    "q = np.zeros((4,4,4))\n",
    "attempts = 0\n",
    "\n",
    "for n in range(5000):      # it's OK to try running more or fewer attempts\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:     \n",
    "        if np.random.random() < explore:\n",
    "            action = maze.sample_n()\n",
    "            new_state, reward, done = maze.step(action)\n",
    "            q[state[0]][state[1]][action] += reward\n",
    "            if max(new_state) < 4 and min(new_state) >= 0:\n",
    "                q[state[0]][state[1]][action] += discount * max(q[new_state[0]][new_state[1]])\n",
    "            state = new_state\n",
    "        else:\n",
    "            state, _, done = maze.step(np.argmax(q[state[0]][state[1]]))\n",
    "            \n",
    "        if max(q[0][0]) > 0:\n",
    "            explore = 0\n",
    "            \n",
    "    if explore > 0:\n",
    "        attempts += 1\n",
    "\n",
    "print('attempts required to converge: ', attempts)\n",
    "maze.convergence()\n",
    "Maze.print_q(q,mode='rewards')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
