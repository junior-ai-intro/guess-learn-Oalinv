{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 8. A Random Walk While Taking Notes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. A random walk while taking notes.**\n",
    "\n",
    "When we took our random walk, we ignored everything that happened except for one event: stumbling accross the exit.\n",
    "\n",
    "What if we paid attention, and learned from our mistakes?\n",
    "\n",
    "Let's start thinking about the maze in terms of machine learning: _the art of accumulating knowledge by learning from mistakes_.\n",
    "\n",
    "We have already seen that, when we explore the maze, it gives us feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze import Maze\n",
    "\n",
    "# take several random walks through the maze\n",
    "maze = Maze()\n",
    "print(maze)\n",
    "for i in range(10):\n",
    "    state = maze.reset()      # go back to the initial state\n",
    "    done = False\n",
    "    print('\\n=== walk number ' + str(i) + ' =======================================================')\n",
    "    while not done:\n",
    "        action = maze.sample()\n",
    "        initial_state = state\n",
    "        state, reward, done = maze.step(action)\n",
    "        print('started at:', initial_state, '| moved:', action, '| reward/penalty =',reward, '| done?', done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to learn from those results, we would to _remember_ our rewards or penalties. We would need to store something, someplace... like taking notes in class (unless you don't take notes; if you don't take notes, it's like taking notes as if you did take notes).\n",
    "\n",
    "If we were doing this by hand, what would we write in our notebook?\n",
    "\n",
    "...well, we only know two things:\n",
    "- our __state__ (that is, our x,y position)\n",
    "- the result of taking an __action__ when starting from our __state__.\n",
    "\n",
    "For example, here is the result of moving north immediately, and the notes we might take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = maze.reset()\n",
    "print('NOTE: initial state =', initial_state, '| attempting to move North...')\\\n",
    "\n",
    "new_state, reward, done = maze.step('N')\n",
    "print('NOTE: new_state =', new_state, '| reward/penalty =', reward, '| done ?', done)\n",
    "\n",
    "print('NOTE: Moving North from (0,0) is a bad idea!')\n",
    "print('NOTE: Why is it bad? Because I got a penalty!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so there's that.\n",
    "\n",
    "Seems like we should associate ( state(0,0) + action(N) = bad idea ).\n",
    "\n",
    "_But equally important... there's no need to look for context. A bad idea is a bad idea. It doesn't matter why it's bad, because the only results are: good, bad, indifferent. Machine learning does not code for explicit conditions (like stepping out of bounds); we are only concerned with state transitions and outcomes._\n",
    "\n",
    "That simplifies the problem. We should be able to remember the outcome associated with any __current state__ and __available action__ (this is called a __transition__, because the __action__ causes us to transit from one __state__ to another):\n",
    "<pre>\n",
    "== state =====    == transition =======    == next state =============\n",
    "state is (0,0) -> action is: move North -> state is (0,-1), game over!\n",
    "==============    =====================    ===========================\n",
    "</pre>\n",
    "\n",
    "We need a place to put all of our __states__, __rewards__ and __penalties__ that can store the effect of any given __transition__.\n",
    "\n",
    "To do that, we just need the _dimensions_... like putting a dozen eggs into a carton that is 6x2, or those occasional have-to-be-different cartons that are 4x3.\n",
    "\n",
    "The maze will reveal two things that will help us to discover the _dimensions_ of the maze problem, which are __independent of the fact that it's a maze__. A big maze could be 1000x1000, and a maze that lets you move diagonally could have 8 actions (N, NE, E, SE, S, SW, W, NW). We don't care what those things represent; we just need to know: how many are there?\n",
    "\n",
    "(please re-read that last sentence)\n",
    "\n",
    "One set of dimensions, that we have seen before, is the __action space__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maze.action_space())\n",
    "print('There are',len(maze.action_space()),'possible actions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the other set of dimensions, which our maze also provides, is the __state space__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here are the dimensions of all possible states:',maze.state_space())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE TO THE CURIOUS: it's not strictly necessary that the maze provide the dimensions of the state space. We could discover that just by exploring the space over and over. It's provided here just to simplify the example._\n",
    "\n",
    "So we need to be able to remember the results of any of 4 actions taken in a 4x4 space, which makes: 4x4x4, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     # this library does all kinds of magical things with numbers\n",
    "q = np.zeros((4,4,4))  # don't worry about these details, just go with it\n",
    "print(q)               # everyone calls this a q-table... will explain later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's our notebook... it works like this:\n",
    "<pre>\n",
    "[[[0. 0. 0. 0.]    row 0, col 0, actions 0,1,2,3\n",
    "  [0. 0. 0. 0.]    row 0, col 1, actions 0,1,2,3\n",
    "  [0. 0. 0. 0.]    row 0, col 2, actions 0,1,2,3\n",
    "  [0. 0. 0. 0.]]   row 0, col 3, actions 0,1,2,3\n",
    "\n",
    " [[0. 0. 0. 0.]    row 1, col 0, actions 0,1,2,3\n",
    "  [0. 0. 0. 0.]\n",
    "  [0. 0. 0. 0.]\n",
    "  [0. 0. 0. 0.]]\n",
    "\n",
    " [[0. 0. 0. 0.]    row 2, col 0, actions 0,1,2,3\n",
    "  [0. 0. 0. 0.]\n",
    "  [0. 0. 0. 0.]\n",
    "  [0. 0. 0. 0.]]\n",
    "\n",
    " [[0. 0. 0. 0.]    row 3, col 0, actions 0,1,2,3\n",
    "  [0. 0. 0. 0.]\n",
    "  [0. 0. 0. 0.]\n",
    "  [0. 0. 0. 0.]]]\n",
    "</pre>\n",
    "\n",
    "Let's say we want to remember that going north right away is a bad idea...\n",
    "\n",
    "Recall that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('state  =', maze.reset())\n",
    "print('result =', maze.step('N'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means _when I was in state (0,0), and chose action 'N\", I got a reward of -1._\n",
    "\n",
    "Or, the quality of the action 'N' from state (0,0) is, well, pretty bad (the name 'q' stands for 'quality').\n",
    "\n",
    "For convenience, let's convert our actions to numbers, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a helpful function that you may need...\n",
    "# it converts N,S,E,W to 0,1,2,3\n",
    "\n",
    "def index_of_action(action):\n",
    "    return maze.action_space().index(action)\n",
    "\n",
    "# let's see how that works\n",
    "for action in maze.action_space():\n",
    "    print(action, index_of_action(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And store our penalty from moving North in our __q-table__ like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q[0][0][index_of_action('N')] = -1   # the dimensions are [state_row][state_col][action]\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can remember past results, we can enforce our one-and-only decision rule: _don't do bad things_.\n",
    "\n",
    "(or: select from among the moves that have the highest available reward, based on past experience)\n",
    "\n",
    "If we were to store the __rewards__ or __penalties__ from every possible initial move, we would get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the row & col do not change...\n",
    "# these are the results of 4 transitions,\n",
    "# all starting from row = 0, col = 0.\n",
    "q[0][0][index_of_action('N')] = -1\n",
    "q[0][0][index_of_action('S')] = 0\n",
    "q[0][0][index_of_action('E')] = 0\n",
    "q[0][0][index_of_action('W')] = -1\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This q-table says 'hey, if you are just starting out... don't go north or west!'\n",
    "<hr>\n",
    "***Exercises***<p>\n",
    "    \n",
    "- Build a q-table that learns incrementally from 100,000 walks through the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "maze = Maze()\n",
    "\n",
    "# this converts N,S,E,W to 0,1,2,3\n",
    "def index_of_action(action):\n",
    "    return maze.action_space().index(action)\n",
    "\n",
    "q = np.zeros((4,4,4))\n",
    "for n in range(100000):\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        ####################################################################\n",
    "        #                                                                  #\n",
    "        #  YOUR CODE HERE:                                                 #\n",
    "        #    - get a sample action from the maze                           #\n",
    "        #    - use the action to take a step; capture the return values    #\n",
    "        #    - update the q table by adding the reward to:                 #\n",
    "        #        > row = state[0]                                          #\n",
    "        #        > col = state[1]                                          #\n",
    "        #        > action = index_of_action(whatever action you took)      #\n",
    "        #        > q[row][col][action] += reward                           #\n",
    "        #     - and don't forget to update your state                      #\n",
    "        #                                                                  #\n",
    "        ####################################################################\n",
    "\n",
    "print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
