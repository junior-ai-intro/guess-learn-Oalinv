{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 10. Q-learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Q-learning**\n",
    "\n",
    "_In Q-learning, a form of reinforcement learning, an agent develops an optimal policy based on interactions with an environment; the environment provides a series of state-action-reward sequences without any additional descriptions, labels, context, or rules._\n",
    "\n",
    "Our last attempt at solving the maze is almost a Q-learning algorithm. It developed the policy \"don't repeat your mistakes\" to find a path through the maze, but it was not nearly the _optimal_ policy. An _optimal_ policy would find the exit in the fewest possible steps. When we consider what we can learn from our states, actions, and rewards, we are missing something.\n",
    "\n",
    "Our last exercise was good at exploiting __penalties__, but what about exploiting __rewards__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Find the reward in this q-table:***\n",
    "<pre>\n",
    " =====  =========================\n",
    " state           action\n",
    " =====  =========================\n",
    "            N     S     E     W\n",
    " (0,0)  [-903.    0.    0. -830.]\n",
    " (0,1)  [-253. -215.    0.    0.]\n",
    " (0,2)  [ -58.    0.  -64.    0.]\n",
    " (0,3)  [   0.    0.    0.    0.]\n",
    "\n",
    " (1,0)  [   0.    0. -236. -231.]\n",
    " (1,1)  [   0.    0.    0.    0.]\n",
    " (1,2)  [   0.  -11.    0.  -14.]\n",
    " (1,3)  [  -3.    0.   -2.    0.]\n",
    "\n",
    " (2,0)  [   0.  -62.    0.  -79.]\n",
    " (2,1)  [ -15.   -9.  -10.    0.]\n",
    " (2,2)  [   0.    0.    0.    0.]\n",
    " (2,3)  [   0.   <font color='blue'>+1.</font>   -1.   -3.]   <font color='blue'>< -- we found the exit! from (2,3) -> move S</font>\n",
    "\n",
    " (3,0)  [   0.    0.    0.    0.]\n",
    " (3,1)  [   0.    0.    0.    0.]\n",
    " (3,2)  [   0.    0.    0.    0.]\n",
    " (3,3)  [   0.    0.    0.    0.]\n",
    " </pre>\n",
    "in this table, it took 3,000 random attempts to find the exit once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There is something critically important hiding in that table.***\n",
    "\n",
    "The table cleary says that, staring in state (2,3), a move to the South results in the state (3,3) and a reward of +1. Let's make our own notation for that:\n",
    "<pre>\n",
    "(2,3)[S] = (3,3){+1}\n",
    "</pre>\n",
    "That suggests that __state (2,3) is a pretty good place to be__, because from (2,3) we can acheive a positive reward. How could we use that knowledge to favor actions that put us in (2,3)?\n",
    "\n",
    "Let's look again at the maze, marked up to show (2,3)[S] = (3,3){+1}:\n",
    "<pre>\n",
    "         ...  ...  ...  +++ \n",
    "enter->  (1)  ...  ...  +++ \n",
    "         ...  ...  ...  +++ \n",
    "\n",
    "         ...  +++  ...  ... \n",
    "         ...  +++  ...  ... \n",
    "         ...  +++  ...  ... \n",
    "\n",
    "         ...  ...  +++  2,3 \n",
    "         ...  ...  +++  [S]  <-this is a good state\n",
    "         ...  ...  +++   +1 \n",
    "\n",
    "         +++  +++  ...  ... \n",
    "         +++  +++  ...  3,3  <-exit\n",
    "         +++  +++  ...  ... \n",
    "\n",
    "</pre>\n",
    "That view of the maze raises an interesting question: if we are exploring, how can we _exploit_ the reward that is available _if and when we arrive at state (2,3)?_\n",
    "\n",
    "Well... how do we ever arrive at state (2,3)? In this maze, the only way is: (1,3)[S] = (2,3){0}:\n",
    "<pre>\n",
    "         ...  ...  ...  +++ \n",
    "enter->  (1)  ...  ...  +++ \n",
    "         ...  ...  ...  +++ \n",
    "\n",
    "         ...  +++  ...  1,3 \n",
    "         ...  +++  ...  [S]  <-this is a totally boring, uninteresting state\n",
    "         ...  +++  ...  -0- \n",
    "\n",
    "         ...  ...  +++  2,3 \n",
    "         ...  ...  +++  [S]  <-this is a good state\n",
    "         ...  ...  +++   +1 \n",
    "\n",
    "         +++  +++  ...  ... \n",
    "         +++  +++  ...  3,3  <-exit\n",
    "         +++  +++  ...  ... \n",
    "</pre>\n",
    "Wait a minute... how can (1,3) be totally boring, if it can lead us to (2,3)?\n",
    "\n",
    "Let's take a closer look at (1,3) and (2,3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Maze' has no attribute 'print_q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8f5e17af6221>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mMaze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Maze' has no attribute 'print_q'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "maze = Maze()\n",
    "\n",
    "def sample(maze):\n",
    "    action = maze.sample()                    # this returns N,S,E,W\n",
    "    return maze.action_space().index(action)  # this converts to 0,1,2,3\n",
    "\n",
    "# build a q-table that finds the exit once\n",
    "q = np.zeros((4,4,4)) \n",
    "stop = False\n",
    "while not stop:\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = sample(maze)                         \n",
    "        new_state, reward, done = maze.step(action)  \n",
    "        q[state[0]][state[1]][action] += reward \n",
    "        state = new_state \n",
    "        if reward > 0:\n",
    "            stop = True\n",
    "\n",
    "Maze.print_q(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
