{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align = 'center'>Guessing Games</h1>\n",
    "<h3 align = 'center'>machine learning, one step at a time</h3>\n",
    "<h3 align = 'center'>Step 10. Q-learning</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Q-learning**\n",
    "\n",
    "_In Q-learning, a form of reinforcement learning, an agent develops an optimal policy based on interactions with an environment; the environment provides a series of state-action-reward sequences without any additional descriptions, labels, context, or rules._\n",
    "\n",
    "Our last attempt at solving the maze is almost a Q-learning algorithm. It developed the policy \"don't repeat your mistakes\" to find a path through the maze, but it was not nearly the _optimal_ policy. An _optimal_ policy would find the exit in the fewest possible steps. When we consider what we can learn from our states, actions, and rewards, we are missing something.\n",
    "\n",
    "Our last exercise was good at exploiting __penalties__, but what about exploiting __rewards__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Find the reward in this q-table:***\n",
    "<pre>\n",
    "=====  ================================\n",
    "state         N       S       E       W\n",
    "\n",
    "(0,0)      -839       0       0    -866\n",
    "(0,1)      -241    -283       0       0\n",
    "(0,2)       -67       0     -53       0\n",
    "(0,3)         0       0       0       0\n",
    "\n",
    "(1,0)         0       0    -224    -227\n",
    "(1,1)         0       0       0       0\n",
    "(1,2)         0     -16       0     -10\n",
    "(1,3)        -3       0      -6       0\n",
    "\n",
    "(2,0)         0     -73       0     -53\n",
    "(2,1)       -12     -12     -12       0\n",
    "(2,2)         0       0       0       0\n",
    "(2,3)         0       __1__      -1      -1  < -- there is the +1!\n",
    "\n",
    "(3,0)         0       0       0       0\n",
    "(3,1)         0       0       0       0\n",
    "(3,2)         0       0       0       0\n",
    "(3,3)         0       0       0       0\n",
    "\n",
    " </pre>\n",
    "in this table, it took 3,000 random attempts to find the exit once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There is something critically important hiding in that table.***\n",
    "\n",
    "The table cleary says that, staring in state (2,3), a move to the South results in the state (3,3) and a reward of +1. Let's make our own notation for that:\n",
    "<pre>\n",
    "(2,3)[S] = (3,3){+1}\n",
    "</pre>\n",
    "That suggests that __state (2,3) is a pretty good place to be__, because from (2,3) we can acheive a positive reward. How could we use that knowledge to favor actions that put us in (2,3)?\n",
    "\n",
    "Let's look again at the maze, marked up to show (2,3)[S] = (3,3){+1}:\n",
    "<pre>\n",
    "         ...  ...  ...  +++ \n",
    "enter->  (1)  ...  ...  +++ \n",
    "         ...  ...  ...  +++ \n",
    "\n",
    "         ...  +++  ...  ... \n",
    "         ...  +++  ...  ... \n",
    "         ...  +++  ...  ... \n",
    "\n",
    "         ...  ...  +++  2,3 \n",
    "         ...  ...  +++  [S]  <-this is a good state\n",
    "         ...  ...  +++   +1 \n",
    "\n",
    "         +++  +++  ...  ... \n",
    "         +++  +++  ...  3,3  <-exit\n",
    "         +++  +++  ...  ... \n",
    "\n",
    "</pre>\n",
    "That view of the maze raises an interesting question: if we are exploring, how can we _exploit_ the reward that is available _if and when we arrive at state (2,3)?_\n",
    "\n",
    "Well... how do we ever arrive at state (2,3)? In this maze, the only way is: (1,3)[S] = (2,3){0}:\n",
    "<pre>\n",
    "         ...  ...  ...  +++ \n",
    "enter->  (1)  ...  ...  +++ \n",
    "         ...  ...  ...  +++ \n",
    "\n",
    "         ...  +++  ...  1,3 \n",
    "         ...  +++  ...  [S]  <-this is a totally boring, uninteresting state\n",
    "         ...  +++  ...  -0- \n",
    "\n",
    "         ...  ...  +++  2,3 \n",
    "         ...  ...  +++  [S]  <-this is a good state\n",
    "         ...  ...  +++   +1 \n",
    "\n",
    "         +++  +++  ...  ... \n",
    "         +++  +++  ...  3,3  <-exit\n",
    "         +++  +++  ...  ... \n",
    "</pre>\n",
    "Wait a minute... how can (1,3) be totally boring, if it can lead us to (2,3)?\n",
    "\n",
    "Let's take a closer look at (1,3) and (2,3), by generating a q-table that has one reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====  ================================\n",
      "state         N       S       E       W\n",
      "\n",
      "(0,0)      -728       0       0    -711\n",
      "(0,1)      -201    -191       0       0\n",
      "(0,2)       -46       0     -47       0\n",
      "(0,3)         0       0       0       0\n",
      "-----  --------------------------------\n",
      "(1,0)         0       0    -177    -190\n",
      "(1,1)         0       0       0       0\n",
      "(1,2)         0     -10       0     -10\n",
      "(1,3)        -5       0      -6       0\n",
      "-----  --------------------------------\n",
      "(2,0)         0     -58       0     -55\n",
      "(2,1)       -11     -11     -15       0\n",
      "(2,2)         0       0       0       0\n",
      "(2,3)         0       1      -1      -2\n",
      "-----  --------------------------------\n",
      "(3,0)         0       0       0       0\n",
      "(3,1)         0       0       0       0\n",
      "(3,2)         0       0       0       0\n",
      "(3,3)         0       0       0       0\n",
      "-----  --------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "maze = Maze()\n",
    "\n",
    "def sample(maze):\n",
    "    action = maze.sample()                    # this returns N,S,E,W\n",
    "    return maze.action_space().index(action)  # this converts to 0,1,2,3\n",
    "\n",
    "# build a q-table that finds the exit once\n",
    "q = np.zeros((4,4,4)) \n",
    "stop = False\n",
    "while not stop:\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = sample(maze)                         \n",
    "        new_state, reward, done = maze.step(action)  \n",
    "        q[state[0]][state[1]][action] += reward \n",
    "        state = new_state \n",
    "        if reward > 0:\n",
    "            stop = True\n",
    "\n",
    "Maze.print_q(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at (2,3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1. -1. -2.]\n"
     ]
    }
   ],
   "source": [
    "print(q[2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have +1 as the second entry, indicating a reward moving [S] to (3,3), which is the exit.\n",
    "\n",
    "What about (1,3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.  0. -6.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(q[1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have 0 as the second entry, indicating no penalty or reward for moving [S] to (2,3).\n",
    "\n",
    "Let's say we are exploring the maze __after we have solved it once__, and we find our way to (1,3), and we randomly consider the action [S]. At that point, if we were to step [S] from (1,3), we could notice that our new state (2,3) has a maximum reward of +1.\n",
    "\n",
    "In other words, for any step, we could ask _what is the maximum known reward of the resulting state, if we were to proceed with that step?_\n",
    "\n",
    "Instead of taking note that (1,3)[S] = (2,3){0}, we could try __(1,3)[S] = (2,3){max(2,3)}__, which says __if we move South from (1,3), we can reasonable expect to take advantage of the maximum known reward available at the resulting state (2,3)__.\n",
    "\n",
    "It's like saying _I know there are cookies in the kitchen, so if a move puts me in the kitchen, I can expect to get a cookie._\n",
    "\n",
    "What is the maximum known reward at state (2,3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum known reward at (2,3) = 1.0\n"
     ]
    }
   ],
   "source": [
    "print('maximum known reward at (2,3) =', max(q[2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's +1... because we found the exit from (2,3) on a previous attempt. When we make notes about (1,3)[S], we should take that into account. We should exploit the rewards, not just the penalties.\n",
    "\n",
    "And once we do that, we will be able to discover that (1,2)[E] = (1,3){max(1,3)}, and so on, backwards, all the way to the starting point:\n",
    "<pre>\n",
    "         0,0  0,1  0,2  +++ \n",
    "enter->  [E]  [E]  [S]  +++ \n",
    "          +1   +1   +1  +++ \n",
    "\n",
    "         ...  +++  1,2  1,3 \n",
    "         ...  +++  [E]  [S] \n",
    "         ...  +++   +1   +1 \n",
    "\n",
    "         ...  ...  +++  2,3 \n",
    "         ...  ...  +++  [S]\n",
    "         ...  ...  +++   +1 \n",
    "\n",
    "         +++  +++  ...  ... \n",
    "         +++  +++  ...  3,3  <-exit\n",
    "         +++  +++  ...  ... \n",
    "</pre>\n",
    "(or something like that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "***Exercises***<p>\n",
    "- Explore the maze by exploiting rewards and penalites.\n",
    "- Alter the program to cause a positive reward to be noted for state (0,0)[E] by taking into account the maximum known reward for each step.\n",
    "- Accumulate enough positive rewards to find an optimal solution to the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====  ================================\n",
      "state         N       S       E       W\n",
      "\n",
      "(0,0)      -490       0       0    -461\n",
      "(0,1)      -129    -130       0       0\n",
      "(0,2)       -29       0     -38       0\n",
      "(0,3)         0       0       0       0\n",
      "-----  --------------------------------\n",
      "(1,0)         0       0    -131    -135\n",
      "(1,1)         0       0       0       0\n",
      "(1,2)         0      -5       0     -15\n",
      "(1,3)        -1       1      -3       0\n",
      "-----  --------------------------------\n",
      "(2,0)         0     -33       0     -33\n",
      "(2,1)        -6      -7      -7       0\n",
      "(2,2)         0       0       0       0\n",
      "(2,3)         0       1      -3       0\n",
      "-----  --------------------------------\n",
      "(3,0)         0       0       0       0\n",
      "(3,1)         0       0       0       0\n",
      "(3,2)         0       0       0       0\n",
      "(3,3)         0       0       0       0\n",
      "-----  --------------------------------\n",
      "===================================\n",
      "         ...  ...  ...  +++ \n",
      "enter->  (1)  (2)  ...  +++ \n",
      "         ...  ...  ...  +++ \n",
      "\n",
      "         ...  +++  ...  ... \n",
      "         ...  +++  ...  ... \n",
      "         ...  +++  ...  ... \n",
      "\n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  ... \n",
      "\n",
      "         +++  +++  ...  ... \n",
      "         +++  +++  ...  ...  <-exit\n",
      "         +++  +++  ...  ... \n",
      "\n",
      "===================================\n",
      "===================================\n",
      "         ...  ...  ...  +++ \n",
      "enter->  (1)  (2)  (3)  +++ \n",
      "         ...  ...  ...  +++ \n",
      "\n",
      "         ...  +++  ...  ... \n",
      "         ...  +++  ...  ... \n",
      "         ...  +++  ...  ... \n",
      "\n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  ... \n",
      "\n",
      "         +++  +++  ...  ... \n",
      "         +++  +++  ...  ...  <-exit\n",
      "         +++  +++  ...  ... \n",
      "\n",
      "===================================\n",
      "===================================\n",
      "         ...  ...  ...  +++ \n",
      "enter->  (1)  (2)  (3)  +++ \n",
      "         ...  ...  ...  +++ \n",
      "\n",
      "         ...  +++  ...  ... \n",
      "         ...  +++  (4)  ... \n",
      "         ...  +++  ...  ... \n",
      "\n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  ... \n",
      "\n",
      "         +++  +++  ...  ... \n",
      "         +++  +++  ...  ...  <-exit\n",
      "         +++  +++  ...  ... \n",
      "\n",
      "===================================\n",
      "===================================\n",
      "         ...  ...  ...  +++ \n",
      "enter->  (1)  (2)  (3)  +++ \n",
      "         ...  ...  ...  +++ \n",
      "\n",
      "         ...  +++  ...  ... \n",
      "         ...  +++  (4)  (5) \n",
      "         ...  +++  ...  ... \n",
      "\n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  ... \n",
      "\n",
      "         +++  +++  ...  ... \n",
      "         +++  +++  ...  ...  <-exit\n",
      "         +++  +++  ...  ... \n",
      "\n",
      "===================================\n",
      "===================================\n",
      "         ...  ...  ...  +++ \n",
      "enter->  (1)  (2)  (3)  +++ \n",
      "         ...  ...  ...  +++ \n",
      "\n",
      "         ...  +++  ...  ... \n",
      "         ...  +++  (4)  (5) \n",
      "         ...  +++  ...  ... \n",
      "\n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  (6) \n",
      "         ...  ...  +++  ... \n",
      "\n",
      "         +++  +++  ...  ... \n",
      "         +++  +++  ...  ...  <-exit\n",
      "         +++  +++  ...  ... \n",
      "\n",
      "===================================\n",
      "===================================\n",
      "         ...  ...  ...  +++ \n",
      "enter->  (1)  (2)  (3)  +++ \n",
      "         ...  ...  ...  +++ \n",
      "\n",
      "         ...  +++  ...  ... \n",
      "         ...  +++  (4)  (5) \n",
      "         ...  +++  ...  ... \n",
      "\n",
      "         ...  ...  +++  ... \n",
      "         ...  ...  +++  (6) \n",
      "         ...  ...  +++  ... \n",
      "\n",
      "         +++  +++  ...  ... \n",
      "         +++  +++  ...  (7)  <-exit\n",
      "         +++  +++  ...  ... \n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from maze import Maze\n",
    "\n",
    "'''\n",
    "=== DO NOT CHANGE CODE STARTING HERE ====== THOU SHALT NOT PASS ================================\n",
    "'''\n",
    "maze = Maze()\n",
    "def sample(maze):\n",
    "    action = maze.sample()                   \n",
    "    return maze.action_space().index(action)\n",
    "\n",
    "def walk(q):\n",
    "    maze = Maze()\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, reward, done = maze.step(np.argmax(q[state[0]][state[1]]))\n",
    "        print(maze)\n",
    "'''\n",
    "=== AND ENDING HERE ======================= THOU SHALT NOT PASS ================================\n",
    "'''\n",
    "q = np.zeros((4,4,4))\n",
    "solved = False\n",
    "while not solved:\n",
    "    state = maze.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = sample(maze)                         \n",
    "        new_state, reward, done = maze.step(action)  \n",
    "        q[state[0]][state[1]][action] += reward     \n",
    "        if max(new_state) < 4 and min(new_state) >=0:\n",
    "            new_q = q[new_state[0]][new_state[1]]\n",
    "            q[state[0]][state[1]][action] += .9 * max(new_q)\n",
    "        state = new_state\n",
    "        solved = True if max(q[0][0]) > 0 else False\n",
    "\n",
    "Maze.print_q(q)    \n",
    "walk(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(q[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
